{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Sheet Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Use Tensorflow to create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as n\n",
    "import keras as k\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is loaded from **Scikit Learn**, using the sklearn.datasets module: http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html.\n",
    "To subsquently process the data, I utilized the sklearn.preprocessing module: http://scikit-learn.org/stable/modules/preprocessing.html. I then implemented sklearn.preprocessing's OneHotEncoder feature to transform the possible species into corresponding matrices of binary integers, since machines cannot comprehend strings of categorical data as we would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "data = load_iris()\n",
    "# derive necessary data\n",
    "x, y_ = data.data, data.target.reshape(-1,1)\n",
    "\n",
    "enc = OneHotEncoder(sparse = False)\n",
    "y = enc.fit_transform(y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Keras** is an API that allows for a high level implementation of a neural network: https://keras.io/. Keras can use **Tensorflow** as it's backend, and does so on default. \n",
    "\n",
    "Tensorflow allows for numerical computation using data flow graphs, which is what is utilized to build the following neural network: https://www.tensorflow.org/. The beginning of the graphs are constant nodes or tensors (the input layer) that are input into a hidden layer of nodes to perform comutations. The ouputs can be passed into another hidden layer of nodes, or left as the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a model with a linear/sequential stack of layers.\n",
    "model = k.models.Sequential()\n",
    "# using the add() method, add input layer of 4 nodes, and a fully connected hidden layer of 16 nodes.\n",
    "model.add(k.layers.Dense(16, input_shape=(4,)))\n",
    "# then apply the sigmoid activation function to that layer. \n",
    "model.add(k.layers.Activation(\"sigmoid\"))\n",
    "# add and fully connect another layer (the output layer) of three nodes.\n",
    "model.add(k.layers.Dense(3))\n",
    "# add the softmax function to the output layer as the activation function. \n",
    "model.add(k.layers.Activation(\"softmax\"))\n",
    "\n",
    "# configure the model for training.\n",
    "# uses the adam optimizer - algorithm used when datasets have a seemingly random pattern.\n",
    "# also uses categorical cross entropy as the loss function because iris a catergorical based dataset.\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split the data into training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would we want to split the data into two seperate sets? If we input the whole datasets and then test it with the same data, the machines predictions will be unproven as it will already have been given the exact data and corresponding classification.\n",
    "\n",
    "Originally I had assumed the application of the Pareto principle would surfice - 80% would be split into the training sets and 20% for the test sets. Upon futher research, I noticed the most common answer was from 60% to 80%: https://www.researchgate.net/post/What_is_the_best_way_to_divide_a_dataset_into_training_and_test_sets , https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio.\n",
    "However, where does this leave us in the terms of our small data set? Bhalla suggests 70% to 100% specifically for the iris data set: http://www.listendata.com/2015/02/splitting-data-into-training-and-test.html.\n",
    "\n",
    "Similar to the data preperation above, I used Scikit Learn's functionality to restructure data for machine learning methods. I used the model_selection module to split the arrays of data into seperate arrays for training and testing: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The training arrays will be 70% of the original, while the test arrays will be 30%.\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "105/105 [==============================] - 1s 7ms/step - loss: 1.1653 - acc: 0.3619\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 1.0433 - acc: 0.4667\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.9855 - acc: 0.4762\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.9390 - acc: 0.3810\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.8961 - acc: 0.6476\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.8573 - acc: 0.6571\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.8156 - acc: 0.7619\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.7808 - acc: 0.7048\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.7475 - acc: 0.7238\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.7147 - acc: 0.8381\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.6902 - acc: 0.8667\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.6578 - acc: 0.8762\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.6335 - acc: 0.8381\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.6085 - acc: 0.9333\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.5849 - acc: 0.8476\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.5648 - acc: 0.9524\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.5481 - acc: 0.9429\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.5312 - acc: 0.8952\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.5114 - acc: 0.8190\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.5008 - acc: 0.9619\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.4842 - acc: 0.9619\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.4684 - acc: 0.9429\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.4543 - acc: 0.9714\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.4413 - acc: 0.9619\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.4290 - acc: 0.9143\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.4178 - acc: 0.9810\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.4128 - acc: 0.9619\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.3951 - acc: 0.9619\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.3851 - acc: 0.9810\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.3703 - acc: 0.9810\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.3620 - acc: 0.9810\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.3538 - acc: 0.9714\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.3408 - acc: 0.9619\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.3346 - acc: 0.9619\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.3257 - acc: 0.9810\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.3147 - acc: 0.9619\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.3058 - acc: 0.9714\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.2994 - acc: 0.9619\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.2908 - acc: 0.9619\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.2864 - acc: 0.9905\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.2736 - acc: 0.9714\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2728 - acc: 0.9619\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.2621 - acc: 0.9810\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.2575 - acc: 0.9810\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.2478 - acc: 0.9714\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.2429 - acc: 0.9810\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.2355 - acc: 0.9810\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.2340 - acc: 0.9714\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.2249 - acc: 0.9810\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.2200 - acc: 0.9905\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.2154 - acc: 0.9810\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2106 - acc: 0.9619\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.2066 - acc: 0.9714\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2008 - acc: 0.9810\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1967 - acc: 0.9810\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1926 - acc: 0.9810\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1864 - acc: 0.9619\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1814 - acc: 0.9714\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1778 - acc: 0.9810\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1757 - acc: 0.9714\n",
      "Epoch 61/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1686 - acc: 0.9714\n",
      "Epoch 62/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1739 - acc: 0.9619\n",
      "Epoch 63/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1644 - acc: 0.9714\n",
      "Epoch 64/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1646 - acc: 0.9714\n",
      "Epoch 65/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1577 - acc: 0.9714\n",
      "Epoch 66/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1545 - acc: 0.9810\n",
      "Epoch 67/100\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 0.1525 - acc: 0.9810\n",
      "Epoch 68/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1506 - acc: 0.9619\n",
      "Epoch 69/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1468 - acc: 0.9714\n",
      "Epoch 70/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1437 - acc: 0.9714\n",
      "Epoch 71/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1414 - acc: 0.9714\n",
      "Epoch 72/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1382 - acc: 0.9810\n",
      "Epoch 73/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1369 - acc: 0.9905\n",
      "Epoch 74/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1365 - acc: 0.9714\n",
      "Epoch 75/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1332 - acc: 0.9810\n",
      "Epoch 76/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1308 - acc: 0.9714\n",
      "Epoch 77/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1274 - acc: 0.9810\n",
      "Epoch 78/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1250 - acc: 0.9619\n",
      "Epoch 79/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1231 - acc: 0.9810\n",
      "Epoch 80/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1221 - acc: 0.9810\n",
      "Epoch 81/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1210 - acc: 0.9810\n",
      "Epoch 82/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1187 - acc: 0.9714\n",
      "Epoch 83/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1171 - acc: 0.9810A: 0s - loss: 0.1014 - acc: 1.00\n",
      "Epoch 84/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1155 - acc: 0.9714\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1124 - acc: 0.9714\n",
      "Epoch 86/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1103 - acc: 0.9810\n",
      "Epoch 87/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1111 - acc: 0.9810\n",
      "Epoch 88/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1085 - acc: 0.9810\n",
      "Epoch 89/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1082 - acc: 0.9810\n",
      "Epoch 90/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1061 - acc: 0.9810\n",
      "Epoch 91/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1051 - acc: 0.9810\n",
      "Epoch 92/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1043 - acc: 0.9714\n",
      "Epoch 93/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0994 - acc: 0.9619\n",
      "Epoch 94/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1027 - acc: 0.9714\n",
      "Epoch 95/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0976 - acc: 0.9714\n",
      "Epoch 96/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1021 - acc: 0.9714\n",
      "Epoch 97/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0964 - acc: 0.9810\n",
      "Epoch 98/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0959 - acc: 0.9905\n",
      "Epoch 99/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0948 - acc: 0.9810\n",
      "Epoch 100/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0944 - acc: 0.9714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dc1464c1d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model with relevant set.\n",
    "model.fit(train_x, train_y, epochs=100, batch_size=1, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
