{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Sheet Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Use Tensorflow to create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as n\n",
    "import keras as k\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is loaded from **Scikit Learn**, using the sklearn.datasets module: http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html.\n",
    "To subsquently process the data, I utilized the sklearn.preprocessing module: http://scikit-learn.org/stable/modules/preprocessing.html. I then implemented sklearn.preprocessing's OneHotEncoder feature to transform the possible species into corresponding matrices of binary integers, since machines cannot comprehend strings of categorical data as we would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "data = load_iris()\n",
    "# derive necessary data\n",
    "x, y_ = data.data, data.target.reshape(-1,1)\n",
    "\n",
    "enc = OneHotEncoder(sparse = False)\n",
    "y = enc.fit_transform(y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Keras** is an API that allows for a high level implementation of a neural network: https://keras.io/. Keras can use **Tensorflow** as it's backend, and does so on default. \n",
    "\n",
    "Tensorflow allows for numerical computation using data flow graphs, which is what is utilized to build the following neural network: https://www.tensorflow.org/. The beginning of the graphs are constant nodes or tensors (the input layer) that are input into a hidden layer of nodes to perform comutations. The ouputs can be passed into another hidden layer of nodes, or left as the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a model with a linear/sequential stack of layers.\n",
    "model = k.models.Sequential()\n",
    "# using the add() method, add input layer of 4 nodes, and a fully connected hidden layer of 16 nodes.\n",
    "model.add(k.layers.Dense(16, input_shape=(4,)))\n",
    "# then apply the sigmoid activation function to that layer. \n",
    "model.add(k.layers.Activation(\"sigmoid\"))\n",
    "# add and fully connect another layer (the output layer) of three nodes.\n",
    "model.add(k.layers.Dense(3))\n",
    "# add the softmax function to the output layer as the activation function. \n",
    "model.add(k.layers.Activation(\"softmax\"))\n",
    "\n",
    "# use the adam optimizer - algorithm used when datasets have a seemingly random pattern.\n",
    "# https://keras.io/optimizers/\n",
    "optimizer = k.optimizers.Adam(lr=0.001)\n",
    "# configure the model for training.\n",
    "# uses categorical cross entropy as the loss function because iris a catergorical based dataset.\n",
    "model.compile(optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split the data into training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would we want to split the data into two seperate sets? If we input the whole datasets and then test it with the same data, the machines predictions will be unproven as it will already have been given the exact data and corresponding classification.\n",
    "\n",
    "Originally I had assumed the application of the Pareto principle would surfice - 80% would be split into the training sets and 20% for the test sets. Upon futher research, I noticed the most common answer was from 60% to 80%: https://www.researchgate.net/post/What_is_the_best_way_to_divide_a_dataset_into_training_and_test_sets , https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio , https://www.researchgate.net/post/Is_there_an_ideal_ratio_between_a_training_set_and_validation_set_Which_trade-off_would_you_suggest. \n",
    "However, where does this leave us in the terms of our small data set? Since the data set is small I decided to stay with the 80% to 20% ratio to ensure the set has been trained extensively enough.\n",
    "\n",
    "Similar to the data preperation above, I used Scikit Learn's functionality to restructure data for machine learning methods. I used the model_selection module to split the arrays of data into seperate arrays for training and testing: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the training arrays will be 80% of the original, while the test arrays will be 20%.\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f819331160>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model with relevant set.\n",
    "# verbose set to 0 - no output while fitting model\n",
    "# epochs - the iteration limit\n",
    "model.fit(train_x, train_y, epochs=500, batch_size=25, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loss: 0.0523\tAccuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model using the test data set.\n",
    "loss, accuracy = model.evaluate(test_x, test_y, verbose=0)\n",
    "\n",
    "# print results.\n",
    "print(\"\\n\\nLoss: %6.4f\\tAccuracy: %6.4f\" % (loss, accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
