{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Sheet Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Use Tensorflow to create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as n\n",
    "import keras as k\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is loaded from **Scikit Learn**, using the sklearn.datasets module: http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html.\n",
    "To subsquently process the data, I utilized the sklearn.preprocessing module: http://scikit-learn.org/stable/modules/preprocessing.html. I then implemented sklearn.preprocessing's OneHotEncoder feature to transform the possible species into corresponding matrices of binary integers, since machines cannot comprehend strings of categorical data as we would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "data = load_iris()\n",
    "# derive necessary data\n",
    "x, y_ = data.data, data.target.reshape(-1,1)\n",
    "\n",
    "enc = OneHotEncoder(sparse = False)\n",
    "y = enc.fit_transform(y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Keras** is an API that allows for a high level implementation of a neural network: https://keras.io/. Keras can use **Tensorflow** as it's backend, and does so on default. \n",
    "\n",
    "Tensorflow allows for numerical computation using data flow graphs, which is what is utilized to build the following neural network: https://www.tensorflow.org/. The beginning of the graphs are constant nodes or tensors (the input layer) that are input into a hidden layer of nodes to perform comutations. The ouputs can be passed into another hidden layer of nodes, or left as the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a model with a linear/sequential stack of layers.\n",
    "model = k.models.Sequential()\n",
    "# using the add() method, add input layer of 4 nodes, and a fully connected hidden layer of 16 nodes.\n",
    "model.add(k.layers.Dense(16, input_shape=(4,)))\n",
    "# then apply the sigmoid activation function to that layer. \n",
    "model.add(k.layers.Activation(\"sigmoid\"))\n",
    "# add and fully connect another layer (the output layer) of three nodes.\n",
    "model.add(k.layers.Dense(3))\n",
    "# add the softmax function to the output layer as the activation function. \n",
    "model.add(k.layers.Activation(\"softmax\"))\n",
    "\n",
    "# configure the model for training.\n",
    "# uses the adam optimizer - algorithm used when datasets have a seemingly random pattern.\n",
    "# also uses categorical cross entropy as the loss function because iris a catergorical based dataset.\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split the data into training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would we want to split the data into two seperate sets? If we input the whole datasets and then test it with the same data, the machines predictions will be unproven as it will already have been given the exact data and corresponding classification.\n",
    "\n",
    "Originally I had assumed the application of the Pareto principle would surfice - 80% would be split into the training sets and 20% for the test sets. Upon futher research, I noticed the most common answer was from 60% to 80%: https://www.researchgate.net/post/What_is_the_best_way_to_divide_a_dataset_into_training_and_test_sets , https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio.\n",
    "However, where does this leave us in the terms of our small data set? Bhalla suggests 70% to 100% specifically for the iris data set: http://www.listendata.com/2015/02/splitting-data-into-training-and-test.html.\n",
    "\n",
    "Similar to the data preperation above, I used Scikit Learn's functionality to restructure data for machine learning methods. I used the model_selection module to split the arrays of data into seperate arrays for training and testing: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The training arrays will be 70% of the original, while the test arrays will be 30%.\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
